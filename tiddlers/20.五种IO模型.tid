created: 20220605132258227
creator: TidGiUser
difficulty: 5.1000000000000005
due: 20220616004300449
grade: 1
history: [{"due":"20220610080836854","interval":0,"difficulty":5,"stability":2,"retrievability":1,"grade":-1,"lapses":0,"reps":1,"review":"20220608080836853"}]
interval: 2
lapses: 0
modified: 20220610004300449
modifier: TidGiUser
reps: 2
retrievability: 0.9
review: 20220610004300449
stability: 5.512139752431654
tags: IO fx ?
title: 20.五种IO模型
type: text/vnd.tiddlywiki

! 1. 同步阻塞式IO
[img[截屏2022-06-05 22.11.39.png]]

! 2. 同步非阻塞式IO
[img[截屏2022-06-05 22.15.19.png]]

简单总结一下：

（1）同步IO是指用户空间(进程或者线程)是主动发起IO请求的一方，系统内核是被动接 受方。异步IO则反过来，系统内核主动发起IO请求的一方，用户空间是被动接受方

（2）那阻塞和非阻塞对比呢：

无论对一个阻塞式的socket还是非阻塞式的socket发起read系统调用，内核在收到系统调用时，都需要经历''两阶段''：

1.等待数据从外设复制到内核缓冲区（一般这一步是DMA干的,可以看看[[00.Linux底层IO(外设-内核)的三种方式]]）

2.从内核复制到用户空间

阻塞式IO:从第一次发起read调用，内核经历两个阶段，直到系统调用返回，用户进程都是阻塞的

阻塞式IO的优点：应用的程序开发非常简单;在阻塞等待数据期间，用户线程挂起， 用户线程基本不会占用CPU资源

缺点：C10K（服务器如何直接10w个并发连接）。由于阻塞的原因，导致一般是一个线程对一个连接（不然有个连接100年没发过数据怎么办？？），高并发情况下会导致：1. 线程太多，线程切换的开销太大（相当于任务总量没变，结果cpu大量的时间却花在了线程切换上） 2. 线程太多，本身也要占内存

非阻塞式IO：发起read调用之后，如果一阶段内核缓冲区数据没准备好，系统调用会直接返回，用户进程解除阻塞状态。如果数据准备好，内核会复制数据，复制过程中用户进程保持阻塞状态。

非阻塞式优点：由于非阻塞了，可以不用一个线程对一个连接，避免了创建大量线程的缺点。

缺点：''系统调用过多''，非阻塞一般是利用很少的线程，去遍历每一个连接，调用IO方法。在高并发的情况下，假如有1000个连接了，每次for就要进行1000次read系统调用，尤其是数据没准备好时，还要进行多次无用的系统调用。

> 从这个角度看，非阻塞并不一定是优势，''连接数少的情况''，非阻塞IO甚至不如阻塞式IO，阻塞式IO只要用一次系统调用，等待内核完成两阶段，返回。非阻塞IO有可能要用多次系统调用

''改进：''
能不能用一个系统调用传1000个客户端给内核''（很多条路复用了这个系统调用''）内核告诉我有几个可以读，然后我再调相应次数的read
''（从N次变为m+1次）''

! 3.多路复用器
IO多路复用指的是一个进程/线程可以同时监视多个文件描述符(含socket连接)，一旦其中的一个或者多个文件描述符可读或者可写，该监听进程/线程能够进行IO事件的查询。

[img[截屏2022-06-05 23.04.31.png]]
在IO多路复用模型中，引入了一种新的系统调用，查询IO的就绪状态。在Linux系统 中，对应的系统调用为select/poll/epoll系统调用

从''用户程序的角度来看''，使用多路复用器的流程如下：

1. 选择器注册：用户进程首先将需要read操作的目标文件描述符(socket 连接)，提前注册到Linux的epoll选择器中，在Java中所对应的选择器类是Selector 类。然后，才可以开启整个IO多路复用模型的轮询流程。

> 如果是select/poll，这一步仅仅是在jvm中创建一个数组，然后把socket放进去。
> 如果是epoll,会调用epoll_create，利用[[mmap|10.传统IO执行流程与零拷贝？]]创建共享空间,利用epoll_ctl将fd存入红黑树，指定监听fd上的哪些[[IO事件|31.Java NIO]]

2. 返回就绪状态的文件描述符列表：用户进程再选择这些就绪状态的socket完成read调用。

> 如果是select/poll，这一步是调用select()，将jvm中的数组，整个传递给内核，内核再轮询所有fd
> 如果是epoll,会调用epoll_wait，等待共享空间中的链表上的fd。这个链表上的fd，是网卡完成IO，数据进入内核缓冲区之后，产生中断，内核继续将红黑树上''监听并且数据就绪的fd''放入链表。（从内核的角度看，像是把一个同步阻塞的select()操作变为异步阻塞了）

总结一下SELECT，POLL缺点：内核不开辟空间保留之前传递过的历史痕迹

1. 重复给内核传递fds,''势必要重复调用用户空间数据复制到内核空间的指令''。解决方案：内核开辟共享空间(mmap)保留fds（epoll会用红黑树保存）

2. 每次select，poll都要重新遍历全量的fd （计租深度知识，中断，callback，增强）。解决方案：将内核中断延伸，数据进入socket的buffer之后，继续将fd存入共享空间中的链表

https://processon.com/diagraming/60ff5ec21e085366ea513125

! 4.信号驱动IO

! 5.异步IO


